{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    },
    "colab": {
      "name": "predicting sales data.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "795bbe4b-51b2-42ec-810a-4f4c18c84f53",
        "_uuid": "e4eb15fdb1237ea12fda77b898eb315b00a205ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya4IleNRaWye",
        "outputId": "5bac5064-3676-4f78-f230-5587969bb4be"
      },
      "source": [
        "# always start with checking out the files!\n",
        "!ls ../input/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '../input/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jaQ3yKaaWyh",
        "outputId": "142bfa02-5f46-4c88-9026-fe70c3f77a0f"
      },
      "source": [
        "# Basic packages\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random as rd # generating random numbers\n",
        "import datetime # manipulating date formats\n",
        "# Viz\n",
        "import matplotlib.pyplot as plt # basic plotting\n",
        "import seaborn as sns # for prettier plots\n",
        "\n",
        "\n",
        "# TIME SERIES\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tsa.api as smt\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scs\n",
        "\n",
        "\n",
        "# settings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "6541e1a6-a353-4709-a1fa-730e0f2a308d",
        "_uuid": "debe15ae99f3596923efc37ce2f609920213be54",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "lpSH0TZCaWyj",
        "outputId": "bb2443fc-da69-42ad-fcec-3f2e99f1a8f9"
      },
      "source": [
        "# Import all of them \n",
        "sales=pd.read_csv(\"../input/sales_train.csv\")\n",
        "\n",
        "# settings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "item_cat=pd.read_csv(\"../input/item_categories.csv\")\n",
        "item=pd.read_csv(\"../input/items.csv\")\n",
        "sub=pd.read_csv(\"../input/sample_submission.csv\")\n",
        "shops=pd.read_csv(\"../input/shops.csv\")\n",
        "test=pd.read_csv(\"../input/test.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4773a5d2c799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import all of them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/sales_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/sales_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "dc6fc0f9-45a9-4146-b88d-d4bddcb224b2",
        "_uuid": "8e1875bb64b6efc577e8b121217e2ded20ea9ce9",
        "id": "uyh1hW0caWyj"
      },
      "source": [
        "#formatting the date column correctly\n",
        "sales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n",
        "# check\n",
        "print(sales.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "dd800a06-41f7-41d2-a402-80ef2cc4ed2d",
        "_uuid": "0ca7c39c5544de1888d111db2450010f85f1a099",
        "collapsed": true,
        "id": "9Ha1f2NcaWyk"
      },
      "source": [
        "# Aggregate to monthly level the required metrics\n",
        "\n",
        "monthly_sales=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n",
        "    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})\n",
        "\n",
        "## Lets break down the line of code here:\n",
        "# aggregate by date-block(month),shop_id and item_id\n",
        "# select the columns date,item_price and item_cnt(sales)\n",
        "# Provide a dictionary which says what aggregation to perform on which column\n",
        "# min and max on the date\n",
        "# average of the item_price\n",
        "# sum of the sales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "986b9168-860f-4ae0-8ed7-c42cb65837fb",
        "_uuid": "3d689df5658dfa3bfbfe531488844a9fdd31d804",
        "id": "YKMq2GHTaWyl"
      },
      "source": [
        "# take a peak\n",
        "monthly_sales.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c8e0a7f3-9a16-46e0-aae3-273fe0f21d0e",
        "_uuid": "a051b790a453f6e28632435a6c30efae02538113",
        "id": "LYS85n2AaWym"
      },
      "source": [
        "# number of items per cat \n",
        "x=item.groupby(['item_category_id']).count()\n",
        "x=x.sort_values(by='item_id',ascending=False)\n",
        "x=x.iloc[0:10].reset_index()\n",
        "x\n",
        "# #plot\n",
        "plt.figure(figsize=(8,4))\n",
        "ax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\n",
        "plt.title(\"Items per Category\")\n",
        "plt.ylabel('# of items', fontsize=12)\n",
        "plt.xlabel('Category', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "68d378e2-2302-4381-8423-ede818fce32e",
        "_uuid": "8dadea026ac25a550cb6725894e1117c67e88757",
        "collapsed": true,
        "id": "cmso6eCaaWyn"
      },
      "source": [
        "Of course, there is a lot more that we can explore in this dataset, but let's dive into the time-series part.\n",
        "\n",
        "# Single series:\n",
        "\n",
        "The objective requires us to predict sales for the next month at a store-item combination.\n",
        "\n",
        "Sales over time of each store-item is a time-series in itself. Before we dive into all the combinations, first let's understand how to forecast for a single series.\n",
        "\n",
        "I've chosen to predict for the total sales per month for the entire company.\n",
        "\n",
        "First let's compute the total sales per month and plot that data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "a783e367-da29-47fd-97be-f3ff756f32fe",
        "_uuid": "95eaf40635366294662b228680cb6e425940c7db",
        "id": "uxKKxWZYaWyo"
      },
      "source": [
        "ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n",
        "ts.astype('float')\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Total Sales of the company')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.plot(ts);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b98fb1f6-f3a2-434f-94c6-af01f3ffdfd4",
        "_uuid": "bee64faeaacd2f60ff85ac8d2b61eea4e80afda8",
        "id": "ay3Dvd_waWyo"
      },
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "plt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\n",
        "plt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5fe94fac-46c3-43c5-b032-705cdfd43726",
        "_uuid": "1a06f1b76571d5d09095148d07ddfa1e4e2002cc",
        "id": "BjsYQRM3aWyp"
      },
      "source": [
        "**Quick observations:**\n",
        "There is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".\n",
        "\n",
        "Let's check that with a quick decomposition into Trend, seasonality and residuals.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b7c4c5fe-8a25-403d-8bb6-fa4f64699c00",
        "_uuid": "611d345c3a3358dd34826c277bd2294247183c0e",
        "id": "_3x3zrkAaWyp"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "# multiplicative\n",
        "res = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"multiplicative\")\n",
        "#plt.figure(figsize=(16,12))\n",
        "fig = res.plot()\n",
        "#fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "68db7d1b-1a74-48d2-96f0-78c8847981bb",
        "_uuid": "80b4215987ff52e4e514b97093a54fc55461430a",
        "id": "SfqnpSEdaWyq"
      },
      "source": [
        "# Additive model\n",
        "res = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"additive\")\n",
        "#plt.figure(figsize=(16,12))\n",
        "fig = res.plot()\n",
        "#fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2176681b-44c0-4b11-9a11-f6172ba3d265",
        "_uuid": "6261f5b777f4d539e383e6928f151b7db4dbf443",
        "collapsed": true,
        "id": "0ObsPPMmaWyq"
      },
      "source": [
        "# R version ported into python  \n",
        "\n",
        "# alas ! rpy2 does not exist in Kaggle kernals :( \n",
        "# from rpy2.robjects import r\n",
        "# def decompose(series, frequency, s_window, **kwargs):\n",
        "#     df = pd.DataFrame()\n",
        "#     df['date'] = series.index\n",
        "#     s = [x for x in series.values]\n",
        "#     length = len(series)\n",
        "#     s = r.ts(s, frequency=frequency)\n",
        "#     decomposed = [x for x in r.stl(s, s_window, **kwargs).rx2('time.series')]\n",
        "#     df['observed'] = series.values\n",
        "#     df['trend'] = decomposed[length:2*length]\n",
        "#     df['seasonal'] = decomposed[0:length]\n",
        "#     df['residual'] = decomposed[2*length:3*length]\n",
        "#     return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7e6f683b-a27d-4a68-9069-e0c713356339",
        "_uuid": "a243f999421ec6d568a781d8a1f9baea720b09db",
        "id": "iEWHelQOaWyr"
      },
      "source": [
        "we assume an additive model, then we can write\n",
        "\n",
        "> yt=St+Tt+Et \n",
        "\n",
        "where yt is the data at period t, St is the seasonal component at period t, Tt is the trend-cycle component at period tt and Et is the remainder (or irregular or error) component at period t\n",
        "Similarly for Multiplicative model,\n",
        "\n",
        "> yt=St  x Tt x Et \n",
        "\n",
        "## Stationarity:\n",
        "\n",
        "![q](https://static1.squarespace.com/static/53ac905ee4b003339a856a1d/t/5818f84aebbd1ac01c275bac/1478031479192/?format=750w)\n",
        "\n",
        "Stationarity refers to time-invariance of a series. (ie) Two points in a time series are related to each other by only how far apart they are, and not by the direction(forward/backward)\n",
        "\n",
        "When a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary.\n",
        "\n",
        "\n",
        "There are multiple tests that can be used to check stationarity.\n",
        "* ADF( Augmented Dicky Fuller Test) \n",
        "* KPSS \n",
        "* PP (Phillips-Perron test)\n",
        "\n",
        "Let's just perform the ADF which is the most commonly used one.\n",
        "\n",
        "Note: [Step by step guide to perform dicky fuller test in Excel](http://www.real-statistics.com/time-series-analysis/stochastic-processes/dickey-fuller-test/)\n",
        "\n",
        "[Another Useful guide](http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016#AR) \n",
        "\n",
        "[good reference](https://github.com/ultimatist/ODSC17/blob/master/Time%20Series%20with%20Python%20(ODSC)%20STA.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0172ae25-5173-4645-960a-cedcb2800cb9",
        "_uuid": "f98bc8fda199838bfa54b1b406e6c7f5023d16bb",
        "id": "W4U3iRhmaWys"
      },
      "source": [
        "# Stationarity tests\n",
        "def test_stationarity(timeseries):\n",
        "    \n",
        "    #Perform Dickey-Fuller test:\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print (dfoutput)\n",
        "\n",
        "test_stationarity(ts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0374ddff-dc1f-4d9b-82f9-f3eff9c9c4b0",
        "_uuid": "a85f4e771a553ff529b46f25c183d33708055378",
        "collapsed": true,
        "id": "jctlwysxaWyt"
      },
      "source": [
        "# to remove trend\n",
        "from pandas import Series as Series\n",
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "    diff = list()\n",
        "    for i in range(interval, len(dataset)):\n",
        "        value = dataset[i] - dataset[i - interval]\n",
        "        diff.append(value)\n",
        "    return Series(diff)\n",
        "\n",
        "# invert differenced forecast\n",
        "def inverse_difference(last_ob, value):\n",
        "    return value + last_ob\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c97fbab1-a301-46bd-95cb-5ba01cdef568",
        "_uuid": "0904a2ab681ac5b3042f5e3d3ba9743955865266",
        "id": "_ttLNIEhaWyt"
      },
      "source": [
        "ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n",
        "ts.astype('float')\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.subplot(311)\n",
        "plt.title('Original')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.plot(ts)\n",
        "plt.subplot(312)\n",
        "plt.title('After De-trend')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "new_ts=difference(ts)\n",
        "plt.plot(new_ts)\n",
        "plt.plot()\n",
        "\n",
        "plt.subplot(313)\n",
        "plt.title('After De-seasonalization')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "new_ts=difference(ts,12)       # assuming the seasonality is 12 months long\n",
        "plt.plot(new_ts)\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9227dec3-bed4-4a12-bc69-563bd68cb3ff",
        "_uuid": "aab34e83d42ceea015ce2f7fe1ace57a115fcd5f",
        "id": "tj3raRFnaWyu"
      },
      "source": [
        "# now testing the stationarity again after de-seasonality\n",
        "test_stationarity(new_ts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "66399279-b53f-4c3b-ad30-68353880a5b0",
        "_uuid": "f6ba95bc505b6de75f94840eb4b1e1ce6ccc90e5",
        "id": "YKQ5wPNSaWyu"
      },
      "source": [
        "### Now after the transformations, our p-value for the DF test is well within 5 %. Hence we can assume Stationarity of the series\n",
        "\n",
        "We can easily get back the original series using the inverse transform function that we have defined above.\n",
        "\n",
        "Now let's dive into making the forecasts!\n",
        "\n",
        "# AR, MA and ARMA models:\n",
        "TL: DR version of the models:\n",
        "\n",
        "MA - Next value in the series is a function of the average of the previous n number of values\n",
        "AR - The errors(difference in mean) of the next value is a function of the errors in the previous n number of values\n",
        "ARMA - a mixture of both.\n",
        "\n",
        "Now, How do we find out, if our time-series in AR process or MA process?\n",
        "\n",
        "Let's find out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "85e12639-f2c2-4ce1-a57a-fba013e0c64c",
        "_uuid": "30302a2f14d1e9a450672504ed3237e10af33d31",
        "collapsed": true,
        "id": "2mBHequ-aWyv"
      },
      "source": [
        "def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n",
        "    if not isinstance(y, pd.Series):\n",
        "        y = pd.Series(y)\n",
        "    with plt.style.context(style):    \n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n",
        "        layout = (3, 2)\n",
        "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
        "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
        "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
        "        qq_ax = plt.subplot2grid(layout, (2, 0))\n",
        "        pp_ax = plt.subplot2grid(layout, (2, 1))\n",
        "        \n",
        "        y.plot(ax=ts_ax)\n",
        "        ts_ax.set_title(title)\n",
        "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n",
        "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n",
        "        sm.qqplot(y, line='s', ax=qq_ax)\n",
        "        qq_ax.set_title('QQ Plot')        \n",
        "        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n",
        "\n",
        "        plt.tight_layout()\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "98e9a6bf-63af-4de5-bc5b-87a2b53749e6",
        "_uuid": "274f0899031c6c8904cc2fc16278210bf60f44cf",
        "id": "O3hw6etyaWyv"
      },
      "source": [
        "# Simulate an AR(1) process with alpha = 0.6\n",
        "np.random.seed(1)\n",
        "n_samples = int(1000)\n",
        "a = 0.6\n",
        "x = w = np.random.normal(size=n_samples)\n",
        "\n",
        "for t in range(n_samples):\n",
        "    x[t] = a*x[t-1] + w[t]\n",
        "limit=12    \n",
        "_ = tsplot(x, lags=limit,title=\"AR(1)process\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e737518c-d725-4ed2-a01d-f82986db65af",
        "_uuid": "b3bfab2ac67a745c9aa1c1c495a958383ebd4b45",
        "collapsed": true,
        "id": "5Z9q6TkUaWyw"
      },
      "source": [
        "## AR(1) process -- has ACF tailing out and PACF cutting off at lag=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c0ae4820-5e6e-4f51-b870-caff9f093a65",
        "_uuid": "bfa6b99d581c1a11248254634fb3932bc0de7a0b",
        "id": "0j_N7XUBaWyw"
      },
      "source": [
        "# Simulate an AR(2) process\n",
        "\n",
        "n = int(1000)\n",
        "alphas = np.array([.444, .333])\n",
        "betas = np.array([0.])\n",
        "\n",
        "# Python requires us to specify the zero-lag value which is 1\n",
        "# Also note that the alphas for the AR model must be negated\n",
        "# We also set the betas for the MA equal to 0 for an AR(p) model\n",
        "# For more information see the examples at statsmodels.org\n",
        "ar = np.r_[1, -alphas]\n",
        "ma = np.r_[1, betas]\n",
        "\n",
        "ar2 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n",
        "_ = tsplot(ar2, lags=12,title=\"AR(2) process\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "789221b6-4c5f-4e22-b740-abd904310050",
        "_uuid": "0e64eb4625e7fed1ea67892cd1ce76f521ed2e43",
        "collapsed": true,
        "id": "s3yfoBpUaWyx"
      },
      "source": [
        "## AR(2) process -- has ACF tailing out and PACF cutting off at lag=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d87cb6df-a332-4ac0-bf2d-df690a4a3510",
        "_uuid": "8b6e8e1fb9d5d32e925a3eb5718bbb3fed09c585",
        "id": "Mlszl3NSaWyx"
      },
      "source": [
        "# Simulate an MA(1) process\n",
        "n = int(1000)\n",
        "# set the AR(p) alphas equal to 0\n",
        "alphas = np.array([0.])\n",
        "betas = np.array([0.8])\n",
        "# add zero-lag and negate alphas\n",
        "ar = np.r_[1, -alphas]\n",
        "ma = np.r_[1, betas]\n",
        "ma1 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n",
        "limit=12\n",
        "_ = tsplot(ma1, lags=limit,title=\"MA(1) process\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8974f547-b74a-4b01-822b-0512bcfbd428",
        "_uuid": "bb9116b36c617672b13e339afd14209c0ea72493",
        "id": "2o1hPvWVaWyx"
      },
      "source": [
        "## MA(1) process -- has ACF cut off at lag=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "266ed44d-a2af-40b2-bc70-1f8c92c97cd4",
        "_uuid": "50d9e7da3491a1da9c88d2da1038651e4dd18931",
        "id": "3yNYZpA3aWyy"
      },
      "source": [
        "# Simulate MA(2) process with betas 0.6, 0.4\n",
        "n = int(1000)\n",
        "alphas = np.array([0.])\n",
        "betas = np.array([0.6, 0.4])\n",
        "ar = np.r_[1, -alphas]\n",
        "ma = np.r_[1, betas]\n",
        "\n",
        "ma3 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n)\n",
        "_ = tsplot(ma3, lags=12,title=\"MA(2) process\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cc105523-c043-41f2-8c33-0e73c2b5eef0",
        "_uuid": "1e3b61a68f1d1840e2d136087ed2daa3991c5e18",
        "id": "WPMaBIapaWyy"
      },
      "source": [
        "## MA(2) process -- has ACF cut off at lag=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c9c8d060-8572-426f-87d9-e786d82ad205",
        "_uuid": "3bb2c3992a9b0fdbe9bc1a4f1dfcf7153e925c31",
        "id": "tAK837F-aWyy"
      },
      "source": [
        "# Simulate an ARMA(2, 2) model with alphas=[0.5,-0.25] and betas=[0.5,-0.3]\n",
        "max_lag = 12\n",
        "\n",
        "n = int(5000) # lots of samples to help estimates\n",
        "burn = int(n/10) # number of samples to discard before fit\n",
        "\n",
        "alphas = np.array([0.8, -0.65])\n",
        "betas = np.array([0.5, -0.7])\n",
        "ar = np.r_[1, -alphas]\n",
        "ma = np.r_[1, betas]\n",
        "\n",
        "arma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n",
        "_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "fce4e806-d217-4b2c-9df6-e38c3d03208b",
        "_uuid": "67306349432a683c926a812bd071915bf5e23e18",
        "id": "2APEH_NmaWyz"
      },
      "source": [
        "# pick best order by aic \n",
        "# smallest aic value wins\n",
        "best_aic = np.inf \n",
        "best_order = None\n",
        "best_mdl = None\n",
        "\n",
        "rng = range(5)\n",
        "for i in rng:\n",
        "    for j in rng:\n",
        "        try:\n",
        "            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n",
        "            tmp_aic = tmp_mdl.aic\n",
        "            if tmp_aic < best_aic:\n",
        "                best_aic = tmp_aic\n",
        "                best_order = (i, j)\n",
        "                best_mdl = tmp_mdl\n",
        "        except: continue\n",
        "\n",
        "\n",
        "print('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9f28bdd-6b6e-4522-9644-f8d6020d830f",
        "_uuid": "e32468dcd2ea44e9477adc212eb7175875dba33b",
        "id": "Fa5ABuypaWyz"
      },
      "source": [
        "## We've correctly identified the order of the simulated process as ARMA(2,2). \n",
        "\n",
        "### Lets use it for the sales time-series.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "4adcd9c6-63eb-41c2-82f3-4bde0ce556ef",
        "_uuid": "43f731d8b664c9531464d8766f1fc911dd69b2e0",
        "id": "KNHxLDtAaWy0"
      },
      "source": [
        "#\n",
        "# pick best order by aic \n",
        "# smallest aic value wins\n",
        "best_aic = np.inf \n",
        "best_order = None\n",
        "best_mdl = None\n",
        "\n",
        "rng = range(5)\n",
        "for i in rng:\n",
        "    for j in rng:\n",
        "        try:\n",
        "            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n",
        "            tmp_aic = tmp_mdl.aic\n",
        "            if tmp_aic < best_aic:\n",
        "                best_aic = tmp_aic\n",
        "                best_order = (i, j)\n",
        "                best_mdl = tmp_mdl\n",
        "        except: continue\n",
        "\n",
        "\n",
        "print('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "62dacf92-a612-4342-812f-8936f45c1dce",
        "_uuid": "733861273519695c485dd59e8cb483e0b91802f3",
        "collapsed": true,
        "id": "9ZY7LBGZaWy0"
      },
      "source": [
        "# Simply use best_mdl.predict() to predict the next values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9f22f870-38b0-44f2-b7cf-90dfc3fefaa6",
        "_uuid": "dd7ffaeba28472d4bc2e8a0b4de8b6613b38b83e",
        "id": "yyHKsB4caWy0"
      },
      "source": [
        "# adding the dates to the Time-series as index\n",
        "ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n",
        "ts.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n",
        "ts=ts.reset_index()\n",
        "ts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5cd6369a-20e7-4586-b9ec-5d804ea64528",
        "_uuid": "d8c35e14d08d580907da6ed43e684ab9b89fb6cf",
        "id": "5b4SClyvaWy1"
      },
      "source": [
        "# Prophet: \n",
        "\n",
        "Recently open-sourced by Facebook research. It's a very promising tool, that is often a very handy and quick solution to the frustrating **flatline** :P\n",
        "\n",
        "![FLATLINE](https://i.stack.imgur.com/fWzyX.jpg)\n",
        "\n",
        "Sure, one could argue that with proper pre-processing and carefully tuning the parameters the above graph would not happen. \n",
        "\n",
        "But the truth is that most of us don't either have the patience or the expertise to make it happen.\n",
        "\n",
        "Also, there is the fact that in most practical scenarios- there is often a lot of time-series that needs to be predicted.\n",
        "Eg: This competition. It requires us to predict the next month sales for the **Store - item level combinations** which could be in the thousands.(ie) predict 1000s of parameters!\n",
        "\n",
        "Another neat functionality is that it follows the typical **sklearn** syntax.\n",
        "\n",
        "At its core, the Prophet procedure is an additive regression model with four main components:\n",
        "* A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n",
        "* A yearly seasonal component modeled using Fourier series.\n",
        "* A weekly seasonal component using dummy variables.\n",
        "* A user-provided list of important holidays.\n",
        "\n",
        "**Resources for learning more about prophet:**\n",
        "* https://www.youtube.com/watch?v=95-HMzxsghY\n",
        "* https://facebook.github.io/prophet/docs/quick_start.html#python-api\n",
        "* https://research.fb.com/prophet-forecasting-at-scale/\n",
        "* https://blog.exploratory.io/is-prophet-better-than-arima-for-forecasting-time-series-fa9ae08a5851"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e0f1d568-e74b-4b4d-970f-17ed78ad6c04",
        "_uuid": "5515d79d56f071c77c955be1ef36de528f953306",
        "id": "g1t7fvV9aWy1"
      },
      "source": [
        "from fbprophet import Prophet\n",
        "#prophet reqiures a pandas df at the below config \n",
        "# ( date column named as DS and the value column as Y)\n",
        "ts.columns=['ds','y']\n",
        "model = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \n",
        "model.fit(ts) #fit the model with your dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1dc15d33-ea9c-47b3-9f10-379e8f259606",
        "_uuid": "d9377c6f2e7537cfaebc606049977154a4cce49a",
        "id": "_7aUH8-uaWy1"
      },
      "source": [
        "# predict for five months in the furure and MS - month start is the frequency\n",
        "future = model.make_future_dataframe(periods = 5, freq = 'MS')  \n",
        "# now lets make the forecasts\n",
        "forecast = model.predict(future)\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c1120a17-8947-42cd-84ee-424f0b60d5d7",
        "_uuid": "695836bdeb4e148f08e3f3349e89bf4345781ca1",
        "id": "zn62S9klaWy2"
      },
      "source": [
        "model.plot(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9821912e-76eb-4997-a4cc-cb111998370b",
        "_uuid": "d3ea5a00ce7d8e7f568a0c900cacc59d58c2893e",
        "id": "4zb54A41aWy2"
      },
      "source": [
        "model.plot_components(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4d72929a-1363-40b1-9394-9d9bc3cbbfcd",
        "_uuid": "50aff39e479cc20c9898b3a9e008eae2bc2eb713",
        "id": "hn4VTephaWy2"
      },
      "source": [
        "Awesome. The trend and seasonality from Prophet look similar to the ones that we had earlier using the traditional methods.\n",
        "\n",
        "## UCM:\n",
        "\n",
        "Unobserved Components Model. The intuition here is similar to that of the prophet. The model breaks down the time-series into its components, trend, seasonal, cycle and regresses them and then predicts the next point for the components and then combines them.\n",
        "\n",
        "Unfortunately, I could not find a good package/code that can perform this model in Python :( \n",
        "\n",
        "R version of UCM: https://bicorner.com/2015/12/28/unobserved-component-models-in-r/\n",
        "\n",
        "# Hierarchical time series:\n",
        "\n",
        "The [Forecasting: principles and practice](https://www.otexts.org/fpp/9/4) , is the ultimate reference book for forecasting by Rob J Hyndman.\n",
        "\n",
        "He lays out the fundamentals of dealing with grouped or Hierarchical forecasts. Consider the following simple scenario.\n",
        "\n",
        "![](https://www.otexts.org/sites/default/files/resize/fpp/images/hts1-550x274.png)\n",
        "\n",
        "Hyndman proposes the following methods to estimate the points in this hierarchy. I've tried to simplify the language to make it more intuitve.\n",
        "\n",
        "### Bottom up approach:\n",
        "* Predict all the base level series using any method, and then just aggregate it to the top.\n",
        "* Advantages: Simple , No information is lost due to aggregation.\n",
        "* Dis-advantages: Lower levels can be noisy\n",
        "\n",
        "### Top down approach:\n",
        "* Predict the top level first. (Eg: predict total sales first)\n",
        "* Then calculate **weights** that denote the proportion of the total sales that needs to be given to the base level forecast(Eg:) the contribution of the item's sales to the total sales \n",
        "* There are different ways of arriving at the \"weights\". \n",
        "    * **Average Historical Proportions** - Simple average of the item's contribution to sales in the past months\n",
        "    * **Proportion of historical averages** - Weight is the ratio of average value of bottom series by the average value of total series (Eg: Weight(item1)= mean(item1)/mean(total_sales))\n",
        "    * **Forecasted Proportions** - Predict the proportion in the future using changes in the past proportions\n",
        "* Use these weights to calcuate the base -forecasts and other levels\n",
        "\n",
        "### Middle out:\n",
        "* Use both bottom up and top down together.\n",
        "* Eg: Consider our problem of predicting store-item level forecasts.\n",
        "    * Take the middle level(Stores) and find forecasts for the stores\n",
        "    * Use bottoms up approach to find overall sales\n",
        "    * Dis-integrate store sales using proportions to find the item-level sales using a top-down approach\n",
        "    \n",
        "### Optimal combination approach:\n",
        "* Predict for all the layers independently\n",
        "* Since, all the layers are independent, they might not be consistent with hierarchy\n",
        "    * Eg: Since the items are forecasted independently, the sum of the items sold in the store might not be equal to the forecasted sale of store  or as Hyndman puts it “aggregate consistent”\n",
        "* Then some matrix calculations and adjustments happen to provide ad-hoc adjustments to the forecast to make them consistent with the hierarchy\n",
        "\n",
        "\n",
        "### Enough with the theory. Lets start making forecasts! :P\n",
        "The problem at hand here, has 22170 items and 60 stores . This indicates that there can be around a **million** individual time-series(item-store combinations) that we need to predict!\n",
        "\n",
        "Configuring each of them would be nearly impossible. Let's use Prophet which does it for us.\n",
        "\n",
        "Starting off with the bottoms up approach.\n",
        "\n",
        "There are some other points to consider here: \n",
        "* Not all stores sell all items\n",
        "* What happens when a new product is introduced? \n",
        "* What if a product is removed off the shelves?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f628232b-2b87-4ecf-98a9-df85b8cfa079",
        "_uuid": "c32a2ee89ed90af6aa786af833a27b3b2570117f",
        "id": "Km4K_0DCaWy3"
      },
      "source": [
        "total_sales=sales.groupby(['date_block_num'])[\"item_cnt_day\"].sum()\n",
        "dates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n",
        "\n",
        "total_sales.index=dates\n",
        "total_sales.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8c62a4c2-c482-417c-ba56-b376584706e7",
        "_uuid": "da06ef3cef98055ec146eb21b2ac4cdc580b73c7",
        "id": "RE530Qd1aWy3"
      },
      "source": [
        "# get the unique combinations of item-store from the sales data at monthly level\n",
        "monthly_sales=sales.groupby([\"shop_id\",\"item_id\",\"date_block_num\"])[\"item_cnt_day\"].sum()\n",
        "# arrange it conviniently to perform the hts \n",
        "monthly_sales=monthly_sales.unstack(level=-1).fillna(0)\n",
        "monthly_sales=monthly_sales.T\n",
        "dates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n",
        "monthly_sales.index=dates\n",
        "monthly_sales=monthly_sales.reset_index()\n",
        "monthly_sales.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ef4ffa1f-170b-421f-9a87-1798cb7ca885",
        "_uuid": "480e0c16e34f95bca30da929861e2c1de14410e4",
        "id": "pkot4z1waWy3"
      },
      "source": [
        "import time\n",
        "start_time=time.time()\n",
        "\n",
        "# Bottoms up\n",
        "# Calculating the base forecasts using prophet\n",
        "# From HTSprophet pachage -- https://github.com/CollinRooney12/htsprophet/blob/master/htsprophet/hts.py\n",
        "forecastsDict = {}\n",
        "for node in range(len(monthly_sales)):\n",
        "    # take the date-column and the col to be forecasted\n",
        "    nodeToForecast = pd.concat([monthly_sales.iloc[:,0], monthly_sales.iloc[:, node+1]], axis = 1)\n",
        "#     print(nodeToForecast.head())  # just to check\n",
        "# rename for prophet compatability\n",
        "    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n",
        "    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n",
        "    growth = 'linear'\n",
        "    m = Prophet(growth, yearly_seasonality=True)\n",
        "    m.fit(nodeToForecast)\n",
        "    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n",
        "    forecastsDict[node] = m.predict(future)\n",
        "    if (node== 10):\n",
        "        end_time=time.time()\n",
        "        print(\"forecasting for \",node,\"th node and took\",end_time-start_time,\"s\")\n",
        "        break\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3a0487c9-1e58-4d37-859a-23776598eac2",
        "_uuid": "e60bf72b1fbbf5c11c1a6e6302a8497ecf2c6dd0",
        "id": "X3aA10psaWy4"
      },
      "source": [
        "~16s for 10 predictions. We need a million predictions. This would not work out.\n",
        "\n",
        "# Middle out:\n",
        "Let's predict for the store level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "458386cd-bd4b-41ad-ac59-a2b0135b89fb",
        "_uuid": "0e1e93358ddc83308b5f16910816977750c8ac87",
        "id": "a-sp5Am2aWy4"
      },
      "source": [
        "monthly_shop_sales=sales.groupby([\"date_block_num\",\"shop_id\"])[\"item_cnt_day\"].sum()\n",
        "# get the shops to the columns\n",
        "monthly_shop_sales=monthly_shop_sales.unstack(level=1)\n",
        "monthly_shop_sales=monthly_shop_sales.fillna(0)\n",
        "monthly_shop_sales.index=dates\n",
        "monthly_shop_sales=monthly_shop_sales.reset_index()\n",
        "monthly_shop_sales.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f812b9fc-a079-4f0f-a19d-5618bf499228",
        "_uuid": "75e7e20609e23bd676cc9781619940a3febf3cab",
        "id": "Btp0jLmSaWy4"
      },
      "source": [
        "start_time=time.time()\n",
        "\n",
        "# Calculating the base forecasts using prophet\n",
        "# From HTSprophet pachage -- https://github.com/CollinRooney12/htsprophet/blob/master/htsprophet/hts.py\n",
        "forecastsDict = {}\n",
        "for node in range(len(monthly_shop_sales)):\n",
        "    # take the date-column and the col to be forecasted\n",
        "    nodeToForecast = pd.concat([monthly_shop_sales.iloc[:,0], monthly_shop_sales.iloc[:, node+1]], axis = 1)\n",
        "#     print(nodeToForecast.head())  # just to check\n",
        "# rename for prophet compatability\n",
        "    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n",
        "    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n",
        "    growth = 'linear'\n",
        "    m = Prophet(growth, yearly_seasonality=True)\n",
        "    m.fit(nodeToForecast)\n",
        "    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n",
        "    forecastsDict[node] = m.predict(future)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "bc342fe1-72cc-46ba-bc52-5bb4fed994fb",
        "_uuid": "cc93cc3b4f09a2e5a0bbaf86cc683f168557e004",
        "collapsed": true,
        "id": "B-9vFA7xaWy5"
      },
      "source": [
        "#predictions = np.zeros([len(forecastsDict[0].yhat),1]) \n",
        "nCols = len(list(forecastsDict.keys()))+1\n",
        "for key in range(0, nCols-1):\n",
        "    f1 = np.array(forecastsDict[key].yhat)\n",
        "    f2 = f1[:, np.newaxis]\n",
        "    if key==0:\n",
        "        predictions=f2.copy()\n",
        "       # print(predictions.shape)\n",
        "    else:\n",
        "       predictions = np.concatenate((predictions, f2), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f6f0ea03-3500-4580-8c3a-b024f8f43a6d",
        "_uuid": "689180c42779b32ab3ea7cffd9f2889e84b0ba4e",
        "id": "qzFwbmyxaWy5"
      },
      "source": [
        "predictions_unknown=predictions[-1]\n",
        "predictions_unknown"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}